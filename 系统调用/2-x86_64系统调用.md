# Linux源码阅读: x86_64系统调用(2)

文章的大部分内容参考自:

- [Coder Heart: Linux Kernel源码阅读 x86-64 系统调用（超详细）编](https://juejin.cn/post/7203681024236355639)

> 注：本文是基于Linux内核6.6.8版本

本文是接着上文《Linux源码阅读: x86_64系统调用(1)》继续往下讲解。


# 1. 系统调用处理程序

这里只会讲解正常的系统调用流程，涉及到调试、追踪、及异常相关的处理，并没有涉及。另外，代码比较长([arch/x86/entry/entry_64.S](https://elixir.bootlin.com/linux/v6.6.8/source/arch/x86/entry/entry_64.S))，全贴上去是为了让大家有一个全局视角，下面我们会逐句来分析。

```assembly
/*
 * 64-bit SYSCALL instruction entry. Up to 6 arguments in registers.
 *
 * This is the only entry point used for 64-bit system calls.  The
 * hardware interface is reasonably well designed and the register to
 * argument mapping Linux uses fits well with the registers that are
 * available when SYSCALL is used.
 *
 * SYSCALL instructions can be found inlined in libc implementations as
 * well as some other programs and libraries.  There are also a handful
 * of SYSCALL instructions in the vDSO used, for example, as a
 * clock_gettimeofday fallback.
 *
 * 64-bit SYSCALL saves rip to rcx, clears rflags.RF, then saves rflags to r11,
 * then loads new ss, cs, and rip from previously programmed MSRs.
 * rflags gets masked by a value from another MSR (so CLD and CLAC
 * are not needed). SYSCALL does not save anything on the stack
 * and does not change rsp.
 *
 * Registers on entry:
 * rax  system call number
 * rcx  return address
 * r11  saved rflags (note: r11 is callee-clobbered register in C ABI)
 * rdi  arg0
 * rsi  arg1
 * rdx  arg2
 * r10  arg3 (needs to be moved to rcx to conform to C ABI)
 * r8   arg4
 * r9   arg5
 * (note: r12-r15, rbp, rbx are callee-preserved in C ABI)
 *
 * Only called from user space.
 *
 * When user can change pt_regs->foo always force IRET. That is because
 * it deals with uncanonical addresses better. SYSRET has trouble
 * with them due to bugs in both AMD and Intel CPUs.
 */

SYM_CODE_START(entry_SYSCALL_64)
	UNWIND_HINT_ENTRY
	ENDBR

	swapgs
	/* tss.sp2 is scratch space. */
	movq	%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)
	SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp
	movq	PER_CPU_VAR(pcpu_hot + X86_top_of_stack), %rsp

SYM_INNER_LABEL(entry_SYSCALL_64_safe_stack, SYM_L_GLOBAL)
	ANNOTATE_NOENDBR

	/* Construct struct pt_regs on stack */
	pushq	$__USER_DS				/* pt_regs->ss */
	pushq	PER_CPU_VAR(cpu_tss_rw + TSS_sp2)	/* pt_regs->sp */
	pushq	%r11					/* pt_regs->flags */
	pushq	$__USER_CS				/* pt_regs->cs */
	pushq	%rcx					/* pt_regs->ip */
SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL)
	pushq	%rax					/* pt_regs->orig_ax */

	PUSH_AND_CLEAR_REGS rax=$-ENOSYS

	/* IRQs are off. */
	movq	%rsp, %rdi
	/* Sign extend the lower 32bit as syscall numbers are treated as int */
	movslq	%eax, %rsi

	/* clobbers %rax, make sure it is after saving the syscall nr */
	IBRS_ENTER
	UNTRAIN_RET

	call	do_syscall_64		/* returns with IRQs disabled */

	/*
	 * Try to use SYSRET instead of IRET if we're returning to
	 * a completely clean 64-bit userspace context.  If we're not,
	 * go to the slow exit path.
	 * In the Xen PV case we must use iret anyway.
	 */

	ALTERNATIVE "", "jmp	swapgs_restore_regs_and_return_to_usermode", \
		X86_FEATURE_XENPV

	movq	RCX(%rsp), %rcx
	movq	RIP(%rsp), %r11

	cmpq	%rcx, %r11	/* SYSRET requires RCX == RIP */
	jne	swapgs_restore_regs_and_return_to_usermode

	/*
	 * On Intel CPUs, SYSRET with non-canonical RCX/RIP will #GP
	 * in kernel space.  This essentially lets the user take over
	 * the kernel, since userspace controls RSP.
	 *
	 * If width of "canonical tail" ever becomes variable, this will need
	 * to be updated to remain correct on both old and new CPUs.
	 *
	 * Change top bits to match most significant bit (47th or 56th bit
	 * depending on paging mode) in the address.
	 */
#ifdef CONFIG_X86_5LEVEL
	ALTERNATIVE "shl $(64 - 48), %rcx; sar $(64 - 48), %rcx", \
		"shl $(64 - 57), %rcx; sar $(64 - 57), %rcx", X86_FEATURE_LA57
#else
	shl	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
	sar	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
#endif

	/* If this changed %rcx, it was not canonical */
	cmpq	%rcx, %r11
	jne	swapgs_restore_regs_and_return_to_usermode

	cmpq	$__USER_CS, CS(%rsp)		/* CS must match SYSRET */
	jne	swapgs_restore_regs_and_return_to_usermode

	movq	R11(%rsp), %r11
	cmpq	%r11, EFLAGS(%rsp)		/* R11 == RFLAGS */
	jne	swapgs_restore_regs_and_return_to_usermode

	/*
	 * SYSCALL clears RF when it saves RFLAGS in R11 and SYSRET cannot
	 * restore RF properly. If the slowpath sets it for whatever reason, we
	 * need to restore it correctly.
	 *
	 * SYSRET can restore TF, but unlike IRET, restoring TF results in a
	 * trap from userspace immediately after SYSRET.  This would cause an
	 * infinite loop whenever #DB happens with register state that satisfies
	 * the opportunistic SYSRET conditions.  For example, single-stepping
	 * this user code:
	 *
	 *           movq	$stuck_here, %rcx
	 *           pushfq
	 *           popq %r11
	 *   stuck_here:
	 *
	 * would never get past 'stuck_here'.
	 */
	testq	$(X86_EFLAGS_RF|X86_EFLAGS_TF), %r11
	jnz	swapgs_restore_regs_and_return_to_usermode

	/* nothing to check for RSP */

	cmpq	$__USER_DS, SS(%rsp)		/* SS must match SYSRET */
	jne	swapgs_restore_regs_and_return_to_usermode

	/*
	 * We win! This label is here just for ease of understanding
	 * perf profiles. Nothing jumps here.
	 */
syscall_return_via_sysret:
	IBRS_EXIT
	POP_REGS pop_rdi=0

	/*
	 * Now all regs are restored except RSP and RDI.
	 * Save old stack pointer and switch to trampoline stack.
	 */
	movq	%rsp, %rdi
	movq	PER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp
	UNWIND_HINT_END_OF_STACK

	pushq	RSP-RDI(%rdi)	/* RSP */
	pushq	(%rdi)		/* RDI */

	/*
	 * We are on the trampoline stack.  All regs except RDI are live.
	 * We can do future final exit work right here.
	 */
	STACKLEAK_ERASE_NOCLOBBER

	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi

	popq	%rdi
	popq	%rsp
SYM_INNER_LABEL(entry_SYSRETQ_unsafe_stack, SYM_L_GLOBAL)
	ANNOTATE_NOENDBR
	swapgs
	sysretq
SYM_INNER_LABEL(entry_SYSRETQ_end, SYM_L_GLOBAL)
	ANNOTATE_NOENDBR
	int3
SYM_CODE_END(entry_SYSCALL_64)
```

# 2. SYSCALL entry开始及结束宏

我们先来看看`SYM_CODE_START`、`SYM_CODE_END`这两个宏：

1. **SYS_CODE_START宏**

    这个宏定义在include/linux/linkage.h文件中：

    ```C
    /* SYM_CODE_START -- use for non-C (special) functions */
    #ifndef SYM_CODE_START
    #define SYM_CODE_START(name)				\
	    SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)
    #endif


    /* SYM_START -- use only if you have to */
    #ifndef SYM_START
    #define SYM_START(name, linkage, align...)		\
	    SYM_ENTRY(name, linkage, align)
    #endif

    /* SYM_ENTRY -- use only if you have to for non-paired symbols */
    #ifndef SYM_ENTRY
    #define SYM_ENTRY(name, linkage, align...)		\
	    linkage(name) ASM_NL				\
	    align ASM_NL					\
	    name:
    #endif

    /* SYM_L_* -- linkage of symbols */
    #define SYM_L_GLOBAL(name)			.globl name

    /* Some toolchains use other characters (e.g. '`') to mark new line in macro */
    #ifndef ASM_NL
    #define ASM_NL		 ;
    #endif
    ```

    在`SYM_START`中又引用了`SYM_A_ALIGN`宏，我们再来看看该宏的实现：

    ```C
    #define SYM_A_ALIGN				ALIGN

    #define ALIGN __ALIGN

    #ifndef __ALIGN
    #define __ALIGN			.balign CONFIG_FUNCTION_ALIGNMENT
    #define __ALIGN_STR		__stringify(__ALIGN)
    #endif
    ```

    因此，对于`SYM_CODE_START(entry_SYSCALL_64)`展开之后为：

    ```C
    .globl entry_SYSCALL_64;             \
    .balign CONFIG_FUNCTION_ALIGNMENT;   \
    entry_SYSCALL_64:
    ```

    这里我们解释下：

    - `SYS_CODE_START`宏定义了一个全局符号，并标明了地址对齐方式及符号的起始地址

    - `.balign`: 在GAS(GNU Assembly)中, `.balign`语句用于对齐下一条指令(instruction)或数据(data)到指定的内存边界。更详细信息，请参考[GAS官方文档](https://sourceware.org/binutils/docs/as/index.html))
    
    - `CONFIG_FUNCTION_ALIGNMENT`在x86_64中默认配置值为16，即16字节对齐。


1. **SYM_CODE_END宏**
   
    这个宏定义在[include/linux/linkage.h](https://elixir.bootlin.com/linux/v6.6.8/source/include/linux/linkage.h#L309)

    ```C
    /* SYM_CODE_END -- the end of SYM_CODE_START_LOCAL, SYM_CODE_START, ... */
    #ifndef SYM_CODE_END
    #define SYM_CODE_END(name)				\
	SYM_END(name, SYM_T_NONE)
    #endif

    /* SYM_END -- use only if you have to */
    #ifndef SYM_END
    #define SYM_END(name, sym_type)				\
	.type name sym_type ASM_NL			\
	.set .L__sym_size_##name, .-name ASM_NL		\
	.size name, .L__sym_size_##name
    #endif

    /* SYM_T_NONE -- type used by assembler to mark entries of unknown type */
    #ifndef SYM_T_NONE
    #define SYM_T_NONE				STT_NOTYPE
    #endif
    ```

    在`SYM_CODE_END`中又引用了`SYM_T_NONE`，我们来看看[STT_NOTYPE](https://elixir.bootlin.com/linux/v6.6.8/source/include/uapi/linux/elf.h#L128)的定义:

    ```C
    #define STT_NOTYPE  0
    ```

    因此，对于`SYM_CODE_END(entry_SYSCALL_64)`展开之后为:

    ```C
    .type entry_SYSCALL_64 0 ;                               \
    .set .L__sym_size_entry_SYSCALL_64, .-entry_SYSCALL_64 ;  \
    .size entry_SYSCALL_64, .L__sym_size_entry_SYSCALL_64
    ```

    这里我们解释下:

    - `SST_NOTYPE`: 在Linux内核（以及ELF文件格式）中，`STT_NOTYPE`是符号表中的一个符号类型，表示“无类型”符号，即该符号没有特定的类型。
  
    - `.L__sym_size_entry_SYSCALL_64`定义了entry_SYSCALL_64段的长度


# 3. UNWIND_HINT_ENTRY宏

 `UNWIND_HINT_ENTRY` 是 Linux 内核中用于 辅助堆栈展开（Stack Unwinding） 的关键宏，主要作用是为 ORC（Oops Rewind Capability）堆栈展开器提供 函数入口点的元数据，确保内核在发生异常（如 BUG/Oops）或调试时，能够正确解析堆栈信息。

> ps: 上面是Deepseek关于“UNWIND_HINT_ENTRY作用”的回答

## 3.1 核心作用

1. **标记函数入口点**

    用于标注内核中特殊的函数入口（如系统调用、中断处理、内核与用户态切换的代码），明确标识这些位置的堆栈状态（例如堆栈指针的位置），帮助 ORC 展开器确定从哪里开始解析堆栈。

1. **生成ORC元数据**

    ORC 是一种轻量级堆栈展开方案（替代复杂的 DWARF）。`UNWIND_HINT_ENTRY`会在编译时生成紧凑的元数据（存储在 .orc_unwind 段），描述函数入口的堆帧信息，例如：

      - 堆栈指针（SP）的初始偏移量

      - 是否终止当前堆栈帧（end=0 表示未终止）

1. **确保堆栈展开可靠性**

    在内核的底层代码（如汇编实现的系统调用入口）中，堆栈布局可能与常规函数不同。UNWIND_HINT_ENTRY 通过显式标记，避免 ORC 展开器因堆栈状态异常而解析失败。


## 3.2 典型应用场景

- 系统调用入口（如 entry_SYSCALL_64）

- 中断处理函数

- 内核与用户态切换的边界代码

- 其他需要显式堆栈状态描述的底层函数。

## 3.3 代码分析

这个宏定义在[arch/x86/include/asm/unwind_hints.h](https://elixir.bootlin.com/linux/v6.6.8/source/arch/x86/include/asm/unwind_hints.h):

```C
.macro UNWIND_HINT_ENTRY
	VALIDATE_UNRET_BEGIN
	UNWIND_HINT_END_OF_STACK
.endm

.macro UNWIND_HINT_END_OF_STACK
	UNWIND_HINT type=UNWIND_HINT_TYPE_END_OF_STACK
.endm
```

**对VALIDATE_UNRET_BEGIN展开**

我们可以在include/linux/objtool.h找到`VALIDATE_UNRET_BEGIN`宏的定义：

```C
/*
    * Use objtool to validate the entry requirement that all code paths do
    * VALIDATE_UNRET_END before RET.
    *
    * NOTE: The macro must be used at the beginning of a global symbol, otherwise
    * it will be ignored.
    */
.macro VALIDATE_UNRET_BEGIN
#if defined(CONFIG_NOINSTR_VALIDATION) && \
    (defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_SRSO))
.Lhere_\@:
    .pushsection .discard.validate_unret
    .long	.Lhere_\@ - .
    .popsection
#endif
.endm
```

1. `.pushsection`和`.popsection`伪指令

    `.pushsection` 和 `.popsection` 伪指令通常需要配对使用。 `.pushsection` 指令会将之后的代码push 到指定的 section 中, `.popsection` 伪指令用于结束push。


    ![pushsection](https://raw.githubusercontent.com/ivanzz1001/linux-kernel-learning/master/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/image/pushsection.jpg)

    参看:

    - [Using as](https://sourceware.org/binutils/docs/as/index.html)

    - [GNU AS汇编器](https://zhuanlan.zhihu.com/p/533821145)


1. `.pushsection .discard.validate_unret`

    - 作用：将后续数据放入 `.discard.validate_unret`段

    - 背景

      - `.discard` 段在链接时被丢弃，但编译阶段会被工具（如 objtool）解析

      - 该段专用于存储 返回路径验证元数据

1. 总结

    `.pushsection`和`.popsection`用于临时切换到一个特定的段（这里是.discard.validate_unret），在其中写入数据，然后恢复原来的段。`.long`指令用于写入一个32位的整数值，这里的表达式是`.Lhere_\@ - .`，也就是标签`.Lhere_@`的地址减去当前地址。

    由于.pushsection会改变当前的段，这里的地址计算可能涉及不同段的地址空间。在汇编器中，不同的段可能有不同的基地址，因此计算跨段的地址偏移需要谨慎。不过，在这个上下文中，.discard.validate_unret可能是一个特殊的段，其地址相对于当前代码段的位置是已知的，或者在链接时会被处理。


**对UNWIND_HINT_END_OF_STACK展开**

我们可以在arch/x86/include/asm/unwind_hints.h找到`UNWIND_HINT_END_OF_STACK`宏的定义：

```C
.macro UNWIND_HINT_END_OF_STACK
	UNWIND_HINT type=UNWIND_HINT_TYPE_END_OF_STACK
.endm
```

1. 展开UNWIND_HINT_TYPE_END_OF_STACK

    先来看看`UNWIND_HINT_TYPE_END_OF_STACK`,其定义在tools/include/linux/objtool_types.h中：

    ```C
    /*
     * UNWIND_HINT_TYPE_END_OF_STACK: The end of the kernel stack unwind before
     * hitting user entry, boot code, or fork entry (when there are no pt_regs
     * available).
     */
    #define UNWIND_HINT_TYPE_END_OF_STACK	1
    ```

1. 展开UNWIND_HINT

    可以在include/linux/objtool.h文件的第93行找到该宏:

    ```C
    /*
     * In asm, there are two kinds of code: normal C-type callable functions and
     * the rest.  The normal callable functions can be called by other code, and
     * don't do anything unusual with the stack.  Such normal callable functions
     * are annotated with the ENTRY/ENDPROC macros.  Most asm code falls in this
     * category.  In this case, no special debugging annotations are needed because
     * objtool can automatically generate the ORC data for the ORC unwinder to read
     * at runtime.
     *
     * Anything which doesn't fall into the above category, such as syscall and
     * interrupt handlers, tends to not be called directly by other functions, and
     * often does unusual non-C-function-type things with the stack pointer.  Such
     * code needs to be annotated such that objtool can understand it.  The
     * following CFI hint macros are for this type of code.
     *
     * These macros provide hints to objtool about the state of the stack at each
     * instruction.  Objtool starts from the hints and follows the code flow,
     * making automatic CFI adjustments when it sees pushes and pops, filling out
     * the debuginfo as necessary.  It will also warn if it sees any
     * inconsistencies.
     */
    .macro UNWIND_HINT type:req sp_reg=0 sp_offset=0 signal=0
    .Lhere_\@:
	    .pushsection .discard.unwind_hints
		    /* struct unwind_hint */
		    .long .Lhere_\@ - .
		    .short \sp_offset
		    .byte \sp_reg
		    .byte \type
		    .byte \signal
		    .balign 4
	    .popsection
    .endm
    ```

    这里通过`.pushsection`切换到了`.discard.unwind_hints`段，并在该段构造了一个`struct unwind_hint`结构，我们可以从tools/include/linux/objtool_types.h找到该结构：

    ```C
    /*
     * This struct is used by asm and inline asm code to manually annotate the
     * location of registers on the stack.
     */
    struct unwind_hint {
	    u32		ip;
	    s16		sp_offset;
	    u8		sp_reg;
	    u8		type;
	    u8		signal;
    };
    ```

    在 Linux 内核的汇编代码中，`.macro UNWIND_HINT` 宏用于生成栈展开提示信息（Unwind Hints），帮助调试器和内核的异常处理机制正确解析栈帧结构。下面简单介绍一下`UNWIND_HINT`宏各个参数的含义:

    ```assembly
    .macro UNWIND_HINT type:req sp_reg=0 sp_offset=0 signal=0
    ```
    
    如下是参数说明:

      - type:req：必需参数，表示栈帧类型（如函数入口、出口或中间状态）

      - sp_reg（默认0）：栈指针（SP）对应的寄存器编号，遵循[DWARF](https://dwarfstd.org/)规范的寄存器编码 

      - sp_offset（默认 0）：栈指针的偏移量

      - signal（默认 0）：标记是否为信号处理帧


# 4. ENDBR指令

`ENDBR`这也是一个宏，定义在arch/x86/include/asm/ibt.h中：

```C
#ifdef CONFIG_X86_64
#define ENDBR	endbr64
#else
#define ENDBR	endbr32
#endif
```
endbr64 是 Intel CET（Control-Flow Enforcement Technology，控制流强制技术） 中引入的一条指令，主要用于防范 ROP（Return-Oriented Programming） 和 JOP（Jump-Oriented Programming） 等基于控制流劫持的攻击。以下是其核心作用和工作原理：

## 4.1 核心作用

- 防御控制流劫持攻击

  endbr64 通过强制间接跳转/调用（如函数指针、虚函数调用）必须跳转到合法目标地址，阻止攻击者利用内存漏洞（如缓冲区溢出）篡改控制流，执行恶意代码。

- 标记合法分支目标

  该指令被放置在函数的入口地址，表示此位置是一个合法的间接分支目标。如果间接跳转的目标地址没有 endbr64 指令，CPU 会触发异常，终止程序执行。

## 4.2 工作原理

1. **启用 CET 保护**

    - 操作系统和 CPU 需支持 CET（如 Intel 11代+ CPU、Linux Kernel 5.18+）。

    - 编译器（如 GCC、Clang）在编译时通过 `-fcf-protection=full` 选项插入 endbr64 指令。

1. **指令行为**

    - 正常执行：endbr64 在直接调用（如 call 或 jmp）时表现为空操作（NOP），不影响正常程序流。

    - 间接跳转检查: 当程序通过间接跳转（如 jmp [rax] 或 call [rbx]）跳转到某个地址时，CPU 会检查目标地址是否以 endbr64 开头

      - 如果是，正常执行

      - 如果不是，触发 #CP（Control Protection） 异常，终止进程

## 4.3 代码示例

```assembly
; 合法函数入口（由编译器自动插入）
my_function:
    endbr64          ; 标记为合法间接跳转目标
    push rbp
    mov rbp, rsp
    ...
    ret

; 攻击者试图跳转到非法地址（无 endbr64）
attack:
    jmp [hacked_address]  ; 若 hacked_address 处无 endbr64，触发异常
```

## 4.4 技术细节

1. **指令编码**

    endbr64 的机器码为 `F3 0F 1E FA`（x86-64 架构)

1. **兼容性**

    - 旧 CPU 会将其视为 NOP，不影响兼容性

    - endbr32 用于 32 位模式


1. **应用场景**

    - 函数入口点（如通过函数指针调用的位置）

    - 虚函数表（vtable）中的方法入口

    - 动态加载的代码（需显式标记合法跳转目标）
    

## 4.5 实际意义

1. **提升软件安全性**

    通过硬件级检查，有效缓解 ROP/JOP 攻击，减少内存漏洞被利用的风险。

1. **现代软件支持**

    - Linux/Windows 已默认启用 CET 保护。

    - 编译器自动插入 endbr64，开发者通常无需手动编写。

## 4.6 总结

endbr64 是硬件辅助的安全指令，用于确保间接控制流跳转的合法性，是防御内存攻击（如 ROP）的重要机制。它在现代操作系统和编译器的协作下，为软件提供了底层安全加固。


# 5. swapgs指令

关于`swapgs`指令，我们来看一下[Intel SDM](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html) Volume 2B中对其的描述:

```text
SWAPGS exchanges the current GS base register value with the value contained in MSR address C0000102H (IA32_KERNEL_GS_BASE).
The SWAPGS instruction is a privileged instruction intended for use by system software.

When using SYSCALL to implement system calls, there is no kernel stack at the OS entry point. Neither is there a straightforward 
method to obtain a pointer to kernel structures from which the kernel stack pointer could be read. Thus, the kernel cannot save 
general purpose registers or reference memory.

By design, SWAPGS does not require any general purpose registers or memory operands. No registers need to be saved before using the
instruction. SWAPGS exchanges the CPL 0 data pointer from the IA32_KERNEL_GS_BASE MSR with the GS base register. The kernel can then
use the GS prefix on normal memory references to access kernel data structures. Similarly, when the OS kernel is entered using an
interrupt or exception (where the kernel stack is already set up), SWAPGS can be used to quickly get a pointer to the kernel data
structures.
```

该指令会交换当前 GS 基址寄存器和 IA32_KERNEL_GS_BASE 寄存器的值，交换后 GS 基址寄存器会指向内核的数据结构。


## 5.1 Deepseek关于swapgs指令的说明

swapgs 是 x86-64 架构中的一条特权指令，主要用于 内核态与用户态切换时快速访问每 CPU（per-CPU）数据。以下是其核心作用及工作原理的逐步解释：

### 5.1.1 核心作用

快速切换 GS 寄存器的基地址。

- 用户态：GS 寄存器通常未被使用，或由应用程序自定义（如 TLS 数据）

- 内核态：GS 基地址指向 当前 CPU 的内核私有数据区（如 struct cpu_entry_area，包含栈、IST 栈、TSS 等）

- `swapgs` 在用户态与内核态切换时，交换用户态和内核态的 GS 基地址，确保内核代码能正确访问当前 CPU 的私有数据

> ps: 我们可以在arch/x86/include/asm/cpu_entry_area.h找到struct cpu_entry_area的定义

### 5.1.2 典型应用场景

1. **系统调用入口**

    - 进入内核态时，通过 swapgs 将 GS 基地址切换为内核的每 CPU 数据区

    - 退出内核态时，再次使用 swapgs 恢复用户态的 GS 基地址

1. **中断/异常处理**

    - 若中断发生在用户态，处理程序需通过 swapgs 切换到内核的 GS 基地址

    - 若中断发生在内核态，无需执行 swapgs（GS 已指向正确数据区）

### 5.1.3 工作原理

1. **GS基地址的存储**

    - 用户态 GS 基地址：由 MSR_GS_BASE 寄存器保存

    - 内核态 GS 基地址：由 MSR_KERNEL_GS_BASE 寄存器保存

1. **swapgs指令行为**

    执行 swapgs 时，CPU 会交换 MSR_GS_BASE 和 MSR_KERNEL_GS_BASE 的值，实现 GS 基地址的快速切换。

1. **操作流程示例(系统调用入口)**

    ```assembly
    ; 用户态通过 syscall 进入内核
    syscall_entry:
        swapgs                  ; 切换 GS 基地址到内核态
        mov %rsp, gs:0x10       ; 使用 gs 访问内核栈地址（gs:0x10 指向当前 CPU 的内核栈）
        ...                     ; 执行内核代码
        swapgs                  ; 退出前恢复用户态 GS 基地址
        sysretq
    ```

### 5.1.4 关键注意事项

1. **对称性**

    - 必须成对使用：进入内核时使用一次`swapgs`，退出时必须再执行一次以恢复状态

    - 若不对称(如漏写swapgs)，会导致 GS 基地址错误，引发崩溃或安全漏洞

1. **内核代码的安全性**

    - 漏洞案例: CVE-2019-1125（因未正确处理 swapgs 导致信息泄漏）

    - 需确保内核代码在 所有路径（包括异常分支）正确执行 swapgs

1. **与中断上下文的关系**

    - 若中断发生在 内核态，无需执行 swapgs（GS 已指向内核数据）

    - 若中断发生在 用户态，必须执行 swapgs 以切换到内核 GS 基地址


### 5.1.5 性能优势

- 避免显式保存/恢复：通过硬件支持的快速交换，减少上下文切换开销

- 每 CPU 数据访问优化：直接通过 gs 段寄存器访问当前 CPU 的数据结构（如 current_task），无需额外计算。

### 5.1.6 总结

swapgs 是 x86-64 内核实现高效态切换的核心指令，通过硬件支持的 GS 基地址交换，确保内核能快速访问每 CPU 数据，同时降低上下文切换开销。其正确使用是内核稳定性和安全性的关键之一

# 6. 切换到内核栈

接下来的三行代码是切换到内核栈:

```assembly
/* tss.sp2 is scratch space. */
movq	%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)
SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp
movq	PER_CPU_VAR(pcpu_hot + X86_top_of_stack), %rsp
```

## 6.1 保存用户空间栈指针

如下代码将用户空间栈指针保存到`cpu_tss_rw.sp2`位置:

```assembly
/* tss.sp2 is scratch space. */
movq	%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)
```

1. **PER_CPU_VAR宏**

    `PER_CPU_VAR`定义在arch/x86/include/asm/percpu.h中:

    ```C
    #ifdef CONFIG_SMP
    #define PER_CPU_VAR(var)	%__percpu_seg:var
    #else /* ! SMP */
    #define PER_CPU_VAR(var)	var
    #endif	/* SMP */
    ```

    当前我们基本都是多CPU架构，因此对应的是`%__percpu_seg:var`。
    
    再来看`__percpu`的定义，其也是在arch/x86/include/asm/percpu.h中:

    ```C
    #ifdef CONFIG_X86_64
    #define __percpu_seg		gs
    #else
    #define __percpu_seg		fs
    #endif
    ```
    由此可知，`%__percpu_seg:var`表示通过GS段寄存器来寻址每CPU数据区域。

1. **cpu_tss_rw**

    `cpu_tss_rw`的声明是在arch/x86/include/asm/processor.h中:

    ```C
    struct tss_struct {
	    /*
	     * The fixed hardware portion.  This must not cross a page boundary
	     * at risk of violating the SDM's advice and potentially triggering
	     * errata.
	     */
	    struct x86_hw_tss	x86_tss;

	    struct x86_io_bitmap	io_bitmap;
    } __aligned(PAGE_SIZE);
    
    DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);
    ```

    而其定义是在arch/x86/kernel/process.c中：

    ```C
    /*
     * per-CPU TSS segments. Threads are completely 'soft' on Linux,
     * no more per-task TSS's. The TSS size is kept cacheline-aligned
     * so they are allowed to end up in the .data..cacheline_aligned
     * section. Since TSS's are completely CPU-local, we want them
     * on exact cacheline boundaries, to eliminate cacheline ping-pong.
     */
    __visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {
	    .x86_tss = {
		    /*
		     * .sp0 is only used when entering ring 0 from a lower
		     * privilege level.  Since the init task never runs anything
		     * but ring 0 code, there is no need for a valid value here.
		     * Poison it.
		     */
		    .sp0 = (1UL << (BITS_PER_LONG-1)) + 1,

        #ifdef CONFIG_X86_32
		    .sp1 = TOP_OF_INIT_STACK,
            
		    .ss0 = __KERNEL_DS,
		    .ss1 = __KERNEL_CS,
        #endif
		    .io_bitmap_base	= IO_BITMAP_OFFSET_INVALID,
	   },
    };
    EXPORT_PER_CPU_SYMBOL(cpu_tss_rw);
    ```

    下面我们直接来看看`DEFINE_PER_CPU_PAGE_ALIGNED`宏，其定义在include/linux/percpu-defs.h中:

    ```C
    #define DEFINE_PER_CPU_PAGE_ALIGNED(type, name)				\
	    DEFINE_PER_CPU_SECTION(type, name, "..page_aligned")		\
	    __aligned(PAGE_SIZE)

    #define DEFINE_PER_CPU_SECTION(type, name, sec)				\
	    __PCPU_ATTRS(sec) __typeof__(type) name

    #define __PCPU_ATTRS(sec)						\
	    __percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))	\
	    PER_CPU_ATTRIBUTES

    //定义在arch/ia64/include/asm/percpu.h
    #define PER_CPU_BASE_SECTION ".data..percpu"
    ```

    综上，我们得到DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw)展开之后为:

    ```C
    __attribute__((section(".data..percpu..page_alined"))) struct tss_struct cpu_tss_rw
    ```

1. **TSS_sp2**

    TSS_sp2的定义是在arch/x86/kernel/asm-offsets.c中:

    ```C
    OFFSET(TSS_sp2, tss_struct, x86_tss.sp2);
    ```

    是tss_struct.x86_tss.sp2的偏移值， 我们来看看该结构:

    ```C
    struct x86_hw_tss {
	    u32			reserved1;
	    u64			sp0;
	    u64			sp1;
        
	    /*
	     * Since Linux does not use ring 2, the 'sp2' slot is unused by
	     * hardware.  entry_SYSCALL_64 uses it as scratch space to stash
	     * the user RSP value.
	     */
	    u64			sp2;
        
	    u64			reserved2;
	    u64			ist[7];
	    u32			reserved3;
	    u32			reserved4;
	    u16			reserved5;
	    u16			io_bitmap_base;
        
    } __attribute__((packed));
    ```

1. **总结**

    ```assembly
    movq	%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)
    ```
    上述代码的作用就是: 将当前栈指针（%rsp）保存到当前 CPU 的 TSS（Task State Segment）结构体的 sp2 字段中，确保内核在处理特定场景（如中断嵌套或特权级切换）时，CPU 能正确切换到预设的备用栈

## 6.2 切换到内核页表

`SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp` 是 Linux 内核中用于 切换到内核页表（CR3） 的宏，通常出现在上下文切换（如中断、系统调用）的关键路径中。其核心作用及实现原理如下：

### 6.2.1 核心作用

1. **切换地址空间**

    将 CPU 的 CR3 寄存器设置为 内核页表基地址，确保后续代码在内核地址空间中执行，能够正确访问内核数据结构和代码。


1. **临时寄存器使用**

    通过`scratch_reg=%rsp` 指定使用 `%rsp` 作为临时寄存器，避免切换过程中破坏其他寄存器的值


### 6.2.2 实现原理

内核页表的全局性：

  - 内核页表（init_top_pgt 或 swapper_pg_dir）是 全局共享 的，所有 CPU 在进入内核态时使用相同的页表。

  - 用户进程的 CR3 指向进程私有页表，而内核代码需切换到全局内核页表。


### 6.2.3 实现源代码分析

`SWITCH_TO_KERNEL_CR3`是一个宏，定义在arch/x86/entry/calling.h中:

```assembly
.macro SWITCH_TO_KERNEL_CR3 scratch_reg:req
	ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_PTI
	mov	%cr3, \scratch_reg
	ADJUST_KERNEL_CR3 \scratch_reg
	mov	\scratch_reg, %cr3
.Lend_\@:
.endm
```
下面我们展开该宏各个部分：

1. **X86_FEATURE_PTI**

    这是一个宏，定义在arch/x86/include/asm/cpufeatures.h中:

    ```C
    #define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
    ```

1. **ALTERNATIVE**

    `ALTERNATIVE`也是一个宏，定义在arch/x86/include/asm/alternative.h中:

    ```C
    /* alternative assembly primitive: */
    #define ALTERNATIVE(oldinstr, newinstr, ft_flags)         \
        OLDINSTR(oldinstr, 1)                                 \  // 生成原始指令
        ".pushsection .altinstructions, \"a\"\n"              \  // 进入替代指令元数据段
        ALTINSTR_ENTRY(ft_flags, 1)                           \  // 记录替换条件与元数据
        ".popsection\n"                                       \
        ".pushsection .altinstr_replacement, \"ax\"\n"        \  // 进入替代指令代码段
        ALTINSTR_REPLACEMENT(newinstr, 1)                     \  // 写入新指令
        ".popsection\n"

    #define ALTINSTR_ENTRY(ft_flags, num)					      \
	    " .long 661b - .\n"				/* label           */ \
	    " .long " b_replacement(num)"f - .\n"		/* new instruction */ \
	    " .4byte " __stringify(ft_flags) "\n"		/* feature + flags */ \
	    " .byte " alt_total_slen "\n"			/* source len      */ \
	    " .byte " alt_rlen(num) "\n"			/* replacement len */
    ```

    ALTERNATIVE 宏是 Linux 内核中`动态指令替换框架`的核心组件，用于在启动时根据 CPU 特性或安全补丁需求，将汇编代码中的旧指令（oldinstr）替换为新指令（newinstr）。其核心作用是通过编译时生成的元数据，在内核初始化阶段完成指令的动态修补，实现性能优化、漏洞修复或硬件兼容性适配。

1. **保存CR3到临时寄存器**

    ```assembly
    mov	%cr3, \scratch_reg
    ```

    上述代码用于将当前CR3的值保存到scratch_reg(此处为`%rsp`)

1. **调整CR3使其指向内核页表**

    ```assembly
    ADJUST_KERNEL_CR3 \scratch_reg
    ```

    我们来看`ADJUST_KERNEL_CR3`宏的实现，代码位于arch/x86/entry/calling.h中:

    ```C
    .macro ADJUST_KERNEL_CR3 reg:req
	    ALTERNATIVE "", "SET_NOFLUSH_BIT \reg", X86_FEATURE_PCID
	    /* Clear PCID and "PAGE_TABLE_ISOLATION bit", point CR3 at kernel pagetables: */
	    andq    $(~PTI_USER_PGTABLE_AND_PCID_MASK), \reg
    .endm
    ```
    
    上面`andq`用于清除reg中的`PTI_USER_PGTABLE_AND_PCID_MASK`位


1. **写回CR3，切换到内核页表**

    ```assembly
    mov	\scratch_reg, %cr3
    ```

### 6.2.4 总结

`SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp` 是 x86 内核态切换的核心操作之一，通过临时寄存器 %rsp 快速切换到全局内核页表，保障内核代码的安全执行，同时兼顾性能优化。理解其机制对分析内核上下文切换和内存管理至关重要。


## 6.3 切换当前CPU的内核栈顶

```assembly
movq	PER_CPU_VAR(pcpu_hot + X86_top_of_stack), %rsp
```

当前cpu的内核栈顶信息保存在pcpu_hot.top_of_stack中，上面代码用于切换到当前CPU的内核栈顶。

1. **pcpu_hot结构**

    `pcpu_hot`数据结构定义在arch/x86/include/asm/current.h中:

    ```C
    struct pcpu_hot {
	    union {
		    struct {
			    struct task_struct	*current_task;
			    int			preempt_count;
			    int			cpu_number;
            #ifdef CONFIG_CALL_DEPTH_TRACKING
			    u64			call_depth;
            #endif
			    unsigned long		top_of_stack;
			    void			*hardirq_stack_ptr;
			    u16			softirq_pending;
            #ifdef CONFIG_X86_64
			    bool			hardirq_stack_inuse;
            #else
			    void			*softirq_stack_ptr;
            #endif
		    };
		    u8	pad[64];
	    };
    };
    ```

1. **X86_top_of_stack**

    该变量定义在arch/x86/kernel/asm-offsets.c中，记录了`top_of_stack`字段在pcpu_hot结构中的偏移：

    ```C
    OFFSET(X86_top_of_stack, pcpu_hot, top_of_stack);
    ```

# 7. 标记进入SYSCALL_64内核栈

```assembly
SYM_INNER_LABEL(entry_SYSCALL_64_safe_stack, SYM_L_GLOBAL)
```

通过前面的步骤我们将`%rsp`寄存器指向了当前CPU的内核栈顶。


## 7.1 SYM_INNER_LABEL宏

该宏定义在[inclue/linux/linkage](https://elixir.bootlin.com/linux/v6.6.8/source/include/linux/linkage.h#L206):
```C
/* SYM_INNER_LABEL -- only for labels in the middle of code */
#ifndef SYM_INNER_LABEL
#define SYM_INNER_LABEL(name, linkage)		\
    .type name SYM_T_NONE ASM_NL			\
    SYM_ENTRY(name, linkage, SYM_A_NONE)
#endif

/* SYM_ENTRY -- use only if you have to for non-paired symbols */
#ifndef SYM_ENTRY
#define SYM_ENTRY(name, linkage, align...)		\
    linkage(name) ASM_NL				\
    align ASM_NL					\
    name:
#endif
```

从上面可以看到，`SYM_INNER_LABEL`也是定义一个符号，与`SYS_CODE_START`宏类似，只不过没有标明地址对齐方式。


# 8. ANNOTATE_NOENDBR指令

`ANNOTATE_NOENDBR` 是 Linux 内核中用于 绕过 Intel CET（Control-Flow Enforcement Technology）的 endbr 指令检查 的宏，通常作用于汇编代码中的某些特殊标签，明确告知编译器/硬件：此处不需要插入或校验 endbr 指令。其核心目的是在特定场景下禁用控制流完整性保护，避免误报或兼容性问题。

**核心作用**

1. 抑制 endbr 指令生成

    在启用 CET 的编译环境中，编译器会自动在函数入口插入 endbr64（或 endbr32）指令。通过 ANNOTATE_NOENDBR 标记的代码位置将 不生成 endbr，避免硬件在此处校验间接跳转合法性。

1. 绕过 CET 校验

    对于某些特殊代码路径（如非函数入口的标签、手写汇编的跳转目标），若其不需要被间接调用，使用此宏可避免触发 CET 的 #CP 异常。

# 9. 在栈上构建pt_regs结构

```assembly
	/* Construct struct pt_regs on stack */
	pushq	$__USER_DS				/* pt_regs->ss */
	pushq	PER_CPU_VAR(cpu_tss_rw + TSS_sp2)	/* pt_regs->sp */
	pushq	%r11					/* pt_regs->flags */
	pushq	$__USER_CS				/* pt_regs->cs */
	pushq	%rcx					/* pt_regs->ip */
SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL)
	pushq	%rax					/* pt_regs->orig_ax */
```

## 9.1 代码解析

**保存用户态数据段和栈指针**

```assembly
pushq	$__USER_DS				/* pt_regs->ss */
pushq	PER_CPU_VAR(cpu_tss_rw + TSS_sp2)	/* pt_regs->sp */
```

1. `__USER_DS`

    用户数据段选择符（User Data Segment Selector），对应 pt_regs->ss 字段。尽管 x86_64 中段寄存器的使用被弱化，仍需保存以兼容硬件行为

    宏定义在arch/x86/include/asm/segment.h中:
    ```C
    /*
     * Segment selector values corresponding to the above entries:
     *
     * Note, selectors also need to have a correct RPL,
     * expressed with the +3 value for user-space selectors:
     */
    #define __KERNEL32_CS			(GDT_ENTRY_KERNEL32_CS*8)
    #define __KERNEL_CS			(GDT_ENTRY_KERNEL_CS*8)
    #define __KERNEL_DS			(GDT_ENTRY_KERNEL_DS*8)
    #define __USER32_CS			(GDT_ENTRY_DEFAULT_USER32_CS*8 + 3)
    #define __USER_DS			(GDT_ENTRY_DEFAULT_USER_DS*8 + 3)
    #define __USER_CS			(GDT_ENTRY_DEFAULT_USER_CS*8 + 3)
    #define __CPUNODE_SEG			(GDT_ENTRY_CPUNODE*8 + 3)
    ```
1. `PER_CPU_VAR(cpu_tss_rw + TSS_sp2)`

    从当前 CPU 的 TSS（Task State Segment）中获取 sp2 字段值，即用户态栈指针（pt_regs->sp）。

    在系统调用进入时，用户态栈指针被暂存在 TSS 的 sp2 中（通过 swapgs 和栈切换操作）


**保存用户态标志寄存器和代码段**

```assembly
pushq	%r11					/* pt_regs->flags */
pushq	$__USER_CS				/* pt_regs->cs */
```
- `%r11`：在 syscall 指令执行后，RFLAGS 寄存器的值会被保存到 `%r11`，此处将其压栈作为 pt_regs->flags。

- `__USER_CS`: 用户端选择符(User Code Descriptor)，对应pt_regs->cs字段

**保存用户态指令指针和系统调用号**

```assembly
	pushq	%rcx					/* pt_regs->ip */
SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL)
	pushq	%rax					/* pt_regs->orig_ax */
```

- `%rcx`: syscall指令将返回地址（用户下一条指令的RIP）保存到`%rcx`，此处压栈作为pt_regs->ip。

- `%rax`: 系统调用号通过 %rax 传递，压栈保存为 pt_regs->orig_ax

>ps: 某些架构可能通过其他寄存器传递参数，此处为 x86_64 约定


## 9.2 pt_regs结构布局

struct pt_regs结构定义在arch/x86/include/asm/ptrace.h中：

```C
struct pt_regs {
/*
 * C ABI says these regs are callee-preserved. They aren't saved on kernel entry
 * unless syscall needs a complete, fully filled "struct pt_regs".
 */
	unsigned long r15;
	unsigned long r14;
	unsigned long r13;
	unsigned long r12;
	unsigned long bp;
	unsigned long bx;
/* These regs are callee-clobbered. Always saved on kernel entry. */
	unsigned long r11;
	unsigned long r10;
	unsigned long r9;
	unsigned long r8;
	unsigned long ax;
	unsigned long cx;
	unsigned long dx;
	unsigned long si;
	unsigned long di;
/*
 * On syscall entry, this is syscall#. On CPU exception, this is error code.
 * On hw interrupt, it's IRQ number:
 */
	unsigned long orig_ax;
/* Return frame for iretq */
	unsigned long ip;
	unsigned long cs;
	unsigned long flags;
	unsigned long sp;
	unsigned long ss;
/* top of stack page */
};
```

在 x86_64 内核中，struct pt_regs 的成员顺序需与压栈顺序严格匹配（栈从高地址向低地址生长)：

- 压栈顺序：从 ss 到 orig_ax，对应栈中从高到低地址排列的成员

- 寄存器保存：除上述代码段外，其他寄存器（如 rdi、rsi 等）通常在后续代码中保存


## 9.3 关键技术背景

1. **系统调用入口流程**

    - syscall指令行为

      - 将RIP存入`%rcx`，RFLAGS存入`%r11`

      - 切换到内核态，跳转到entry_SYSCALL_64 入口

    - 栈切换

      内核通过`swapgs`指令和加载`cpu_tss_rw + TSS_sp2`切换到当前CPU的内核栈。

1. **TSS的sp2字段作用**

    - 保存用户栈指针：在通过 syscall 进入内核时，用户态栈指针（%rsp）被暂存到 TSS 的 sp2 字段，以便在构建 pt_regs 时恢复。

    - 隔离性： 每 CPU 的 TSS 独立存储用户栈指针，避免多核竞争

1. **标签entry_SYSCALL_64_after_hwframe**

    - 调试与跟踪：标记系统调用处理中硬件帧（Hardware Frame）后的位置，便于内核调试工具定位

## 9.4 完整流程示例

1. 用户进程执行syscall指令，触发系统调用

1. 硬件自动保存 RIP 到 `%rcx`，RFLAGS 到 `%r11`，并跳转到内核入口 entry_SYSCALL_64

1. 内核入口代码

    - 执行 swapgs 切换到内核 GS 基地址

    - 切换到内核栈（`movq PER_CPU_VAR(cpu_tss_rw + TSS_sp2), %rsp`）

    - 构建 pt_regs：按顺序压入 ss、sp、flags、cs、ip、orig_ax。

1. 后续处理

    - 保存其他寄存器（rdi，rsi等）

    - 调用系统调用处理函数（如 do_syscall_64）

    - 返回用户态前从 pt_regs 恢复寄存器

## 9.5 总结

这段汇编代码是 x86_64 系统调用入口的核心逻辑，通过按顺序压栈构建 struct pt_regs，保存用户态关键寄存器值，为内核正确处理系统调用提供完整的上下文信息。其正确性直接关系到系统调用的可靠性与用户态进程的透明恢复

# 10. 保存及清理相关寄存器

```assembly
PUSH_AND_CLEAR_REGS rax=$-ENOSYS
```

1. **宏作用**

    展开后会将所有通用寄存器压入内核栈（构建完整的 struct pt_regs 上下文），并清除敏感寄存器的值（防止信息泄漏），同时显式设置 %rax 为 -ENOSYS（系统调用不存在错误码）。

1. **$-ENOSYS**

    ENOSYS 是错误号（通常为 38），-ENOSYS 表示系统调用未实现的错误值。此处预设 %rax 为此值，若后续未找到合法系统调用号，直接返回此错误。


## 10.1 代码解析

我们来看看`PUSH_AND_CLEAR_REGS`宏，其定义在arch/x86/entry/calling.h中：

```C
.macro PUSH_AND_CLEAR_REGS rdx=%rdx rcx=%rcx rax=%rax save_ret=0
	PUSH_REGS rdx=\rdx, rcx=\rcx, rax=\rax, save_ret=\save_ret
	CLEAR_REGS
.endm

.macro PUSH_REGS rdx=%rdx rcx=%rcx rax=%rax save_ret=0
	.if \save_ret
	pushq	%rsi		/* pt_regs->si */
	movq	8(%rsp), %rsi	/* temporarily store the return address in %rsi */
	movq	%rdi, 8(%rsp)	/* pt_regs->di (overwriting original return address) */
	.else
	pushq   %rdi		/* pt_regs->di */
	pushq   %rsi		/* pt_regs->si */
	.endif
	pushq	\rdx		/* pt_regs->dx */
	pushq   \rcx		/* pt_regs->cx */
	pushq   \rax		/* pt_regs->ax */
	pushq   %r8		/* pt_regs->r8 */
	pushq   %r9		/* pt_regs->r9 */
	pushq   %r10		/* pt_regs->r10 */
	pushq   %r11		/* pt_regs->r11 */
	pushq	%rbx		/* pt_regs->rbx */
	pushq	%rbp		/* pt_regs->rbp */
	pushq	%r12		/* pt_regs->r12 */
	pushq	%r13		/* pt_regs->r13 */
	pushq	%r14		/* pt_regs->r14 */
	pushq	%r15		/* pt_regs->r15 */
	UNWIND_HINT_REGS

	.if \save_ret
	pushq	%rsi		/* return address on top of stack */
	.endif
.endm

.macro CLEAR_REGS
	/*
	 * Sanitize registers of values that a speculation attack might
	 * otherwise want to exploit. The lower registers are likely clobbered
	 * well before they could be put to use in a speculative execution
	 * gadget.
	 */
	xorl	%esi,  %esi	/* nospec si  */
	xorl	%edx,  %edx	/* nospec dx  */
	xorl	%ecx,  %ecx	/* nospec cx  */
	xorl	%r8d,  %r8d	/* nospec r8  */
	xorl	%r9d,  %r9d	/* nospec r9  */
	xorl	%r10d, %r10d	/* nospec r10 */
	xorl	%r11d, %r11d	/* nospec r11 */
	xorl	%ebx,  %ebx	/* nospec rbx */
	xorl	%ebp,  %ebp	/* nospec rbp */
	xorl	%r12d, %r12d	/* nospec r12 */
	xorl	%r13d, %r13d	/* nospec r13 */
	xorl	%r14d, %r14d	/* nospec r14 */
	xorl	%r15d, %r15d	/* nospec r15 */

.endm
```
xorl 是 x86/x86-64 汇编语言中的 32 位异或指令，属于按位逻辑操作指令。其核心功能是对两个 32 位操作数进行 逐位异或运算，并将结果写入目标操作数。异或运算规则为：对应位相同结果为 0，不同则为 1。

关于为什么要保存及清理这些寄存器，在arch/x86/entry/calling.h中已有较为详细的描述:

```text
/*

 x86 function call convention, 64-bit:
 -------------------------------------
  arguments           |  callee-saved      | extra caller-saved | return
 [callee-clobbered]   |                    | [callee-clobbered] |
 ---------------------------------------------------------------------------
 rdi rsi rdx rcx r8-9 | rbx rbp [*] r12-15 | r10-11             | rax, rdx [**]

 ( rsp is obviously invariant across normal function calls. (gcc can 'merge'
   functions when it sees tail-call optimization possibilities) rflags is
   clobbered. Leftover arguments are passed over the stack frame.)

 [*]  In the frame-pointers case rbp is fixed to the stack frame.

 [**] for struct return values wider than 64 bits the return convention is a
      bit more complex: up to 128 bits width we return small structures
      straight in rax, rdx. For structures larger than that (3 words or
      larger) the caller puts a pointer to an on-stack return struct
      [allocated in the caller's stack frame] into the first argument - i.e.
      into rdi. All other arguments shift up by one in this case.
      Fortunately this case is rare in the kernel.
/*
```
    

# 11. 为调用do_syscall_64准备参数

```assembly
/* IRQs are off. */
movq	%rsp, %rdi
/* Sign extend the lower 32bit as syscall numbers are treated as int */
movslq	%eax, %rsi
```

## 11.1 代码解析

1. do_syscall_64函数原型

    我们可以在arch/x86/include/asm/syscall.h中找到do_syscall_64()函数的原型：

    ```C
    void do_syscall_64(struct pt_regs *regs, int nr);
    ```

    其中，第一个参数是pt_regs指针；第二个参数是系统调用编号。

1. `movq %rsp, %rdi`

    - 作用： 将当前栈指针 %rsp（指向已压栈的 struct pt_regs）作为 第一个参数 传递给内核处理函数。

    - 调用约定：x86_64 使用 %rdi 作为第一个参数，此处为后续调用 do_syscall_64() 或类似函数做准备，传递用户态上下文。

1. `movslq	%eax, %rsi`

    - 作用：将 %eax 中的 32 位系统调用号 符号扩展 为 64 位值，存入 %rsi 作为 第二个参数

    - 符号扩展的意义

      系统调用号在Linux中被定义为int(有符号32位整数)，若用户传递的调用号超过`__NR_syscalls`，内核需将其视为负数错误码(如`-ENOSYS`)，符号扩展确保 64 位环境正确处理该值。

      - 示例：若`%eax = 0xffffffff`(即-1)，movslq后`%rsi=0xffffffffffffffff`

## 11.2 完整上下文流程

到这里，我们再总结一下系统调用完整上下文流程：

1. **用户态触发系统调用**

    用户进程通过`syscall`指令进入内核，硬件自动保存 `%rip` 到 `%rcx`，`%rflags` 到 `%r11`，并跳转到 entry_SYSCALL_64。


1. **构建pt_regs 上下文**

    通过`PUSH_AND_CLEAR_REGS`将用户态寄存器压栈，形成完整的 struct pt_regs 结构体。

1. **参数传递**

    - 通过`%rdi`传递第一个参数：其保存的是一个pt_regs指针

    - 通过`%rsi`传递第二个参数：其保存的是系统调用号

1. **系统调用分派**

    ```C
    // 伪代码逻辑
    if (syscall_nr >= __NR_syscalls) {
        return -ENOSYS; // 直接使用预设的 %rax 值
    } else {
        invoke syscall_table[syscall_nr](pt_regs); // 传递 pt_regs
    }
    ```
说明：根据注释`IRQs are off`，表明此时中断已关闭，避免在处理系统调用期间被中断打断，确保原子性


# 12. IBRS_ENTER和UNTRAIN_RET 

在 Linux 内核中，IBRS_ENTER 和 UNTRAIN_RET 是用于防御 Spectre v2（CVE-2017-5715） 等侧信道攻击(Side Channel Attack)的关键机制，尤其在系统调用入口这类特权级切换的关键路径中。它们通过限制 CPU 的推测执行行为，防止攻击者利用间接分支预测（Indirect Branch Prediction）窃取敏感数据。以下是它们的核心作用和技术细节：

## 12.1 IBRS_ENTER: 启用间接分支限制推测

**作用**

- 防御目标：防止用户态通过`训练间接分支预测`，影响内核态代码的推测执行路径

- 硬件支持：依赖 Intel 的 IBRS（Indirect Branch Restricted Speculation） 特性（需 CPU 支持，且内核启用 CONFIG_CPU_IBRS_ENTRY)

- 行为：设置`MSR_IA32_SPEC_CTRL` MSR寄存器的`IBRS`位，限制间接分支的推测执行仅在当前特权级（CPL）的上下文中进行。

**代码实现**

我们来看`IBRS_ENTER`宏，其在arch/x86/entry/calling.h中定义：

```C
/*
 * IBRS kernel mitigation for Spectre_v2.
 *
 * Assumes full context is established (PUSH_REGS, CR3 and GS) and it clobbers
 * the regs it uses (AX, CX, DX). Must be called before the first RET
 * instruction (NOTE! UNTRAIN_RET includes a RET instruction)
 *
 * The optional argument is used to save/restore the current value,
 * which is used on the paranoid paths.
 *
 * Assumes x86_spec_ctrl_{base,current} to have SPEC_CTRL_IBRS set.
 */
.macro IBRS_ENTER save_reg
#ifdef CONFIG_CPU_IBRS_ENTRY
	ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
	movl	$MSR_IA32_SPEC_CTRL, %ecx

.ifnb \save_reg
	rdmsr
	shl	$32, %rdx
	or	%rdx, %rax
	mov	%rax, \save_reg
	test	$SPEC_CTRL_IBRS, %eax
	jz	.Ldo_wrmsr_\@
	lfence
	jmp	.Lend_\@
.Ldo_wrmsr_\@:
.endif

	movq	PER_CPU_VAR(x86_spec_ctrl_current), %rdx
	movl	%edx, %eax
	shr	$32, %rdx
	wrmsr
.Lend_\@:
#endif
.endm
```

代码解析如下：

1. 参数save_reg

    可选参数，用于指定一个寄存器来保存原始`IA32_SPEC_CTRL` MSR 的值（通常用于嵌套调用场景）。

1. 条件编译与动态代码替换

    ```assembly
    #ifdef CONFIG_CPU_IBRS_ENTRY
        ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
    ```

    - `CONFIG_CPU_IBRS_ENTRY`：内核配置选项，决定是否启用 IBRS 进入内核的防护逻辑

    - `ALTERNATIVE`：动态代码替换宏，若 CPU 支持 X86_FEATURE_KERNEL_IBRS（如支持增强型 IBRS），则替换空操作；否则替换为 jmp .Lend_\@ 跳过后续代码（不启用 IBRS）。

1. 读取并保存原始MSR值(可选)

    ```assembly
    .ifnb \save_reg
        rdmsr
        shl    $32, %rdx
        or     %rdx, %rax
        mov    %rax, \save_reg
        test   $SPEC_CTRL_IBRS, %eax
        jz     .Ldo_wrmsr_\@
        lfence
        jmp    .Lend_\@
    .Ldo_wrmsr_\@:
    .endif
    ```

    - rdmsr: 读取 IA32_SPEC_CTRL MSR 的值到 edx:eax

    - 合并为64位值：通过 shl 和 or 将 edx:eax 合并到 `%rax`，保存到 save_reg 寄存器。

    - 检查 IBRS 位：若已设置 SPEC_CTRL_IBRS，执行 lfence 后跳转结束（避免重复设置）

    - 若未设置：跳转到 .Ldo_wrmsr_\@ 执行写操作

1. 设置新的MSR值

    ```assembly
        movq   PER_CPU_VAR(x86_spec_ctrl_current), %rdx
        movl   %edx, %eax
        shr    $32, %rdx
        wrmsr
    .Lend_\@:
    #endif
    .endm
    ```

    - 加载每 CPU 变量：x86_spec_ctrl_current 存储当前 CPU 的 SPEC_CTRL 配置（包含 IBRS 位）

    - 拆分高低 32 位：%edx 为高 32 位，%eax 为低 32 位。

    - wrmsr：将 %edx:%eax 写入 IA32_SPEC_CTRL MSR，启用 IBRS


## 12.2 UNTRAIN_RET:清空返回地址预测器

UNTRAIN_RET 宏是 Linux 内核中用于防御 返回指令预测攻击（如 Spectre v2、Retbleed）的关键机制，其核心作用是在特权级切换点（如系统调用入口）清空或隔离 CPU 的返回地址预测器（Return Stack Buffer, RSB），防止用户态通过训练分支预测器影响内核的推测执行路径。

**防御目标**

防止用户态通过训练返回地址预测器（Return Stack Buffer, RSB），操控内核的返回指令推测执行路径

- Spectre v2：利用间接分支预测窃取跨特权级数据。

- Retbleed：通过操纵返回指令预测窃取内核数据。

- 其他预测攻击：如用户态训练 RSB 污染内核返回路径

**原理**

插入特定的指令序列（如 lfence 或虚假返回指令），清空 CPU 的返回地址预测缓存，避免跨特权级的预测污染。

**代码实现**

我们可以在arch/x86/include/asm/nospec-branch.h中找到该宏：

```C
/*
 * Mitigate RETBleed for AMD/Hygon Zen uarch. Requires KERNEL CR3 because the
 * return thunk isn't mapped into the userspace tables (then again, AMD
 * typically has NO_MELTDOWN).
 *
 * While retbleed_untrain_ret() doesn't clobber anything but requires stack,
 * entry_ibpb() will clobber AX, CX, DX.
 *
 * As such, this must be placed after every *SWITCH_TO_KERNEL_CR3 at a point
 * where we have a stack but before any RET instruction.
 */
.macro UNTRAIN_RET
#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \
	defined(CONFIG_CALL_DEPTH_TRACKING) || defined(CONFIG_CPU_SRSO)
	VALIDATE_UNRET_END
	ALTERNATIVE_3 "",						\
		      CALL_UNTRAIN_RET, X86_FEATURE_UNRET,		\
		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
		      __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
#endif
.endm
```

1. VALIDATE_UNRET_END

    - 作用：验证此前是否已正确应用 UNRET 或相关缓解措施（如检查 RSB 状态或特定寄存器值），确保安全屏障生效。

    - 实现：可能包含调试断言或硬件检查指令（如 lfence 序列化操作）

1. ALTERNATIVE_3

    根据 CPU 支持的特性动态选择缓解策略：

    - 默认：空操作（无缓解）

    - 选项1： CALL_UNTRAIN_RET（UNRET）

      适用 CPU：支持 X86_FEATURE_UNRET（如部分 Intel/AMD CPU 的软件缓解）。
      行为：插入指令序列清空 RSB（如虚假返回指令 + lfence）

    - 选项 2：call entry_ibpb（IBPB）

      适用 CPU：支持 X86_FEATURE_ENTRY_IBPB（如 AMD 的 Indirect Branch Prediction Barrier）。
      行为：调用 entry_ibpb 函数，执行 wrmsr 设置 IBPB MSR，刷新分支预测器。
      性能影响：较高（每次进入内核都需写 MSR

## 12.3 总结

IBRS_ENTER 和 UNTRAIN_RET 是 Linux 内核应对 Spectre v2 攻击的核心防御措施，通过硬件特性与软件序列的结合，在系统调用入口处构建安全屏障。理解其原理对分析内核安全机制、调试性能问题及优化缓解策略至关重要。


# 13. 调用do_syscall_64

```assembly
call	do_syscall_64		/* returns with IRQs disabled */
```

do_syscall_64 是 Linux 内核中 x86_64 架构处理系统调用的核心函数，其功能涵盖从用户态进入内核的系统调用处理全流程，包括安全防护、参数验证、系统调用分发及退出处理。我们可以在arch/x86/entry/common.c中找到其实现：

```C
__visible noinstr void do_syscall_64(struct pt_regs *regs, int nr)
{
	add_random_kstack_offset();
	nr = syscall_enter_from_user_mode(regs, nr);

	instrumentation_begin();

	if (!do_syscall_x64(regs, nr) && !do_syscall_x32(regs, nr) && nr != -1) {
		/* Invalid system call, but still a system call. */
		regs->ax = __x64_sys_ni_syscall(regs);
	}

	instrumentation_end();
	syscall_exit_to_user_mode(regs);
}
```

## 13.1 代码解析

1. **内核栈随机化**

    ```C
    add_random_kstack_offset();
    ```

    - 作用: 在内核栈上添加随机偏移，防御基于栈布局预测的攻击（如 ROP 攻击）

    - 实现依赖：需启用 CONFIG_RANDOMIZE_KSTACK_OFFSET 编译选项，通过每次系统调用时随机调整栈指针增加攻击难度。

1. **系统调用入口处理**

    ```C
    nr = syscall_enter_from_user_mode(regs, nr);
    ```

    - 功能：从用户模式进入系统调用的通用预处理

      - 审计与跟踪：触发 syscall_enter 跟踪点，供调试或监控工具使用。

      - 安全性检查：运行 seccomp 过滤器，决定是否允许该系统调用。

      - 系统调用号调整：处理 x32 ABI 兼容性，将 x32 调用号映射到 x64 范围。

      - 上下文保存：记录系统调用开始时间（用于统计或调度）。

    - 返回值：可能修改 nr（如过滤非法调用号或兼容转换）

1. **仪器化控制**

    ```C
    instrumentation_begin();
    ```

    - 作用：禁用动态仪器化工具（如 KASAN 内存检测），确保关键代码路径的执行稳定性。

    - 实现：通过宏展开为`__kcsan_disable_current()`等操作，避免检测工具引入额外开销或干扰

1. **系统调用分发与执行**

    ```C
    if (!do_syscall_x64(regs, nr) && !do_syscall_x32(regs, nr) && nr != -1) {
		/* Invalid system call, but still a system call. */
		regs->ax = __x64_sys_ni_syscall(regs);
	}
    ```

    - `do_syscall_x64()`: 处理原生x86_64系统调用

      - 检查 nr 是否在合法范围（`0 ≤ nr < __NR_syscall_compat_max`）

      - 若合法，从sys_call_table获取处理函数并执行，结果存入regs->ax(用户态rax)

    - `do_syscall_x32()`: 处理x32 ABI兼容调用（32位程序在64位内核）

      - 检查 nr 是否属于 x32 范围，并调用对应处理函数。

    - 无效调用处理：若两者均未处理且 `nr != -1`，调用 `__x64_sys_ni_syscall` 返回 -ENOSYS（功能未实现）。

1. **仪器化恢复**

    ```C
    instrumentation_end();
    ```

    - 作用：重新启用仪器化工具，恢复调试或检测功能。

    - 实现：与 instrumentation_begin() 对应，恢复之前的仪器化状态

1. **退出到用户模式**

    ```C
    syscall_exit_to_user_mode(regs);
    ```

    - 功能：处理系统调用返回用户态前的收尾工作：

      - 审计与跟踪：触发 syscall_exit 跟踪点。

      - 信号处理：检查待处理的信号（TIF_SIGPENDING）。

      - 调度检查：若需要重新调度（TIF_NEED_RESCHED），调用调度器。

      - 上下文恢复：从 regs 恢复用户态寄存器（除 RAX 外）。

      - 安全屏障：执行 swapgs 和 sysretq，切换回用户态。

## 13.2 总结

do_syscall_64 作为 x86_64 系统调用的核心分发器，整合了安全防护、兼容性处理、执行逻辑及退出机制，确保用户态请求在内核中高效、安全地处理。其设计体现了 Linux 内核在性能与安全性之间的精细平衡，是多层防御体系的关键组成部分。


# 14. 系统调用返回路径

如下是Linux内核在x86_64架构下处理系统调用返回路径的核心逻辑，其目标是通过`SYSRET`指令快速返回用户态，同时确保安全性。

```
    /*
	 * Try to use SYSRET instead of IRET if we're returning to
	 * a completely clean 64-bit userspace context.  If we're not,
	 * go to the slow exit path.
	 * In the Xen PV case we must use iret anyway.
	 */

	ALTERNATIVE "", "jmp	swapgs_restore_regs_and_return_to_usermode", \
		X86_FEATURE_XENPV

	movq	RCX(%rsp), %rcx
	movq	RIP(%rsp), %r11

	cmpq	%rcx, %r11	/* SYSRET requires RCX == RIP */
	jne	swapgs_restore_regs_and_return_to_usermode

	/*
	 * On Intel CPUs, SYSRET with non-canonical RCX/RIP will #GP
	 * in kernel space.  This essentially lets the user take over
	 * the kernel, since userspace controls RSP.
	 *
	 * If width of "canonical tail" ever becomes variable, this will need
	 * to be updated to remain correct on both old and new CPUs.
	 *
	 * Change top bits to match most significant bit (47th or 56th bit
	 * depending on paging mode) in the address.
	 */
#ifdef CONFIG_X86_5LEVEL
	ALTERNATIVE "shl $(64 - 48), %rcx; sar $(64 - 48), %rcx", \
		"shl $(64 - 57), %rcx; sar $(64 - 57), %rcx", X86_FEATURE_LA57
#else
	shl	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
	sar	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
#endif

	/* If this changed %rcx, it was not canonical */
	cmpq	%rcx, %r11
	jne	swapgs_restore_regs_and_return_to_usermode

	cmpq	$__USER_CS, CS(%rsp)		/* CS must match SYSRET */
	jne	swapgs_restore_regs_and_return_to_usermode

	movq	R11(%rsp), %r11
	cmpq	%r11, EFLAGS(%rsp)		/* R11 == RFLAGS */
	jne	swapgs_restore_regs_and_return_to_usermode

	/*
	 * SYSCALL clears RF when it saves RFLAGS in R11 and SYSRET cannot
	 * restore RF properly. If the slowpath sets it for whatever reason, we
	 * need to restore it correctly.
	 *
	 * SYSRET can restore TF, but unlike IRET, restoring TF results in a
	 * trap from userspace immediately after SYSRET.  This would cause an
	 * infinite loop whenever #DB happens with register state that satisfies
	 * the opportunistic SYSRET conditions.  For example, single-stepping
	 * this user code:
	 *
	 *           movq	$stuck_here, %rcx
	 *           pushfq
	 *           popq %r11
	 *   stuck_here:
	 *
	 * would never get past 'stuck_here'.
	 */
	testq	$(X86_EFLAGS_RF|X86_EFLAGS_TF), %r11
	jnz	swapgs_restore_regs_and_return_to_usermode

	/* nothing to check for RSP */

	cmpq	$__USER_DS, SS(%rsp)		/* SS must match SYSRET */
	jne	swapgs_restore_regs_and_return_to_usermode

	/*
	 * We win! This label is here just for ease of understanding
	 * perf profiles. Nothing jumps here.
	 */
syscall_return_via_sysret:
	IBRS_EXIT
	POP_REGS pop_rdi=0

	/*
	 * Now all regs are restored except RSP and RDI.
	 * Save old stack pointer and switch to trampoline stack.
	 */
	movq	%rsp, %rdi
	movq	PER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp
	UNWIND_HINT_END_OF_STACK

	pushq	RSP-RDI(%rdi)	/* RSP */
	pushq	(%rdi)		/* RDI */

	/*
	 * We are on the trampoline stack.  All regs except RDI are live.
	 * We can do future final exit work right here.
	 */
	STACKLEAK_ERASE_NOCLOBBER

	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi

	popq	%rdi
	popq	%rsp
SYM_INNER_LABEL(entry_SYSRETQ_unsafe_stack, SYM_L_GLOBAL)
	ANNOTATE_NOENDBR
	swapgs
	sysretq
SYM_INNER_LABEL(entry_SYSRETQ_end, SYM_L_GLOBAL)
	ANNOTATE_NOENDBR
	int3
SYM_CODE_END(entry_SYSCALL_64)
```

下面我们分步解析：

## 14.1 核心逻辑：条件检查与路劲选择

1. **Xen PV环境检查**

    ```assembly
    /*
	 * Try to use SYSRET instead of IRET if we're returning to
	 * a completely clean 64-bit userspace context.  If we're not,
	 * go to the slow exit path.
	 * In the Xen PV case we must use iret anyway.
	 */

	ALTERNATIVE "", "jmp	swapgs_restore_regs_and_return_to_usermode", \
		X86_FEATURE_XENPV
    ```

    - 作用：若CPU支持Xen半虚拟化(X86_FEATURE_XENPV)，直接跳转到swapgs_restore_regs_and_return_to_usermode(使用`IRET`路径)

    - 原因：Xen PV 环境下 SYSRET 可能无法正确工作，需强制使用 IRET

1. **SYSRET基本条件检查**

    ```assembly
    movq RCX(%rsp), %rcx       ;将栈上保存的用户态 RCX 值加载到 %rcx 寄存器
    movq RIP(%rsp), %r11       ;将栈上保存的用户态 RIP 值加载到 %r11 寄存器
    cmpq %rcx, %r11
    jne swapgs_restore_regs_and_return_to_usermode
    ```

    - 要求：SYSRET 要求 RCX == RIP（RCX 存放用户态返回地址，RIP 是当前返回地址）。

    - 失败处理：若不等，跳转到 IRET 路径。

    对于上述语句，我们再解释一下：

    - `RCX(%rsp)`和`RIP(%rsp)`：

      表示从当前栈指针 %rsp 的偏移位置读取数据。RCX 和 RIP 是预定义的常量，对应 struct pt_regs 中 rcx 和 rip 字段的偏移量

    - `struct pt_regs` 布局

      ```C
      struct pt_regs {
          unsigned long r15;
          unsigned long r14;
          
          ...
	      unsigned long cx;
	      unsigned long ip;
      };
      ```

      当系统调用进入内核时，所有用户态寄存器（包括 RCX 和 RIP）会被压入内核栈，形成 pt_regs 结构

1. **地址规范性检查**

    ```assembly
    /*
	 * On Intel CPUs, SYSRET with non-canonical RCX/RIP will #GP
	 * in kernel space.  This essentially lets the user take over
	 * the kernel, since userspace controls RSP.
	 *
	 * If width of "canonical tail" ever becomes variable, this will need
	 * to be updated to remain correct on both old and new CPUs.
	 *
	 * Change top bits to match most significant bit (47th or 56th bit
	 * depending on paging mode) in the address.
	 */
    #ifdef CONFIG_X86_5LEVEL
	    ALTERNATIVE "shl $(64 - 48), %rcx; sar $(64 - 48), %rcx", \
		    "shl $(64 - 57), %rcx; sar $(64 - 57), %rcx", X86_FEATURE_LA57
    #else
	    shl	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
	    sar	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
    #endif

	/* If this changed %rcx, it was not canonical */
	cmpq	%rcx, %r11
	jne	swapgs_restore_regs_and_return_to_usermode
    ```

    - 目的：验证返回地址是否为规范地址(canonical address)

      - 非规范地址：高位地址位不全为 0 或 1，可能导致 SYSRET 触发 `#GP` 异常

      - 操作：通过移位和符号扩展，检查地址是否规范

      - 失败处理：地址非法，跳转到`IRET`路径

1. **段寄存器与标志位检查**

    ```assembly
    cmpq	$__USER_CS, CS(%rsp)		/* CS must match SYSRET */
	jne	swapgs_restore_regs_and_return_to_usermode

	movq	R11(%rsp), %r11
	cmpq	%r11, EFLAGS(%rsp)		/* R11 == RFLAGS */
	jne	swapgs_restore_regs_and_return_to_usermode

	/*
	 * SYSCALL clears RF when it saves RFLAGS in R11 and SYSRET cannot
	 * restore RF properly. If the slowpath sets it for whatever reason, we
	 * need to restore it correctly.
	 *
	 * SYSRET can restore TF, but unlike IRET, restoring TF results in a
	 * trap from userspace immediately after SYSRET.  This would cause an
	 * infinite loop whenever #DB happens with register state that satisfies
	 * the opportunistic SYSRET conditions.  For example, single-stepping
	 * this user code:
	 *
	 *           movq	$stuck_here, %rcx
	 *           pushfq
	 *           popq %r11
	 *   stuck_here:
	 *
	 * would never get past 'stuck_here'.
	 */
	testq	$(X86_EFLAGS_RF|X86_EFLAGS_TF), %r11
	jnz	swapgs_restore_regs_and_return_to_usermode

	/* nothing to check for RSP */

	cmpq	$__USER_DS, SS(%rsp)		/* SS must match SYSRET */
	jne	swapgs_restore_regs_and_return_to_usermode
    ```

    - 要求：

      - CS 必须指向用户代码段（`__USER_CS`）

      - R11 必须等于 RFLAGS（SYSRET 从 R11 恢复标志寄存器）

      - RFLAGS 中不能设置 RF（Resume Flag）或 TF（Trap Flag），否则可能导致调试陷阱或无限循环。

      - SS 必须指向用户数据段（`__USER_DS`）

    - 失败处理：任一条件不满足，跳转到 IRET 路径

## 14.2 SYSRET快速路径

若所有检查通过，进入 syscall_return_via_sysret 标签，执行以下操作:

1. **关闭间接分支保护**

    ```assembly
    IBRS_EXIT
    ```

    - 作用：若启用了 IBRS（Indirect Branch Restricted Speculation），在此处关闭，减少性能开销

1. **恢复寄存器**

    ```assembly
    POP_REGS pop_rdi=0
    ```

    - 作用：从栈中弹出寄存器值，恢复用户态上下文（pop_rdi=0 表示不弹出 RDI，因其已被其他逻辑处理）。

1. **切换栈并清理**

    ```assembly
    /*
	 * Now all regs are restored except RSP and RDI.
	 * Save old stack pointer and switch to trampoline stack.
	 */
	movq	%rsp, %rdi
	movq	PER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp   ;/* 切换到每 CPU 的 trampoline 栈 */
	UNWIND_HINT_END_OF_STACK

	pushq	RSP-RDI(%rdi)	                          ; /* 保存旧 RSP */
	pushq	(%rdi)		                              ; /* 保存旧 RDI */
    ```

    - 目的：

      - 切换到内核的 trampoline 栈，确保后续操作（如栈清理）不会破坏用户态栈

      - 保存旧的 RSP 和 RDI 到新栈，供后续恢复

1. **安全清理**

    ```assembly
    STACKLEAK_ERASE_NOCLOBBER
    ```

    - 作用：擦除内核栈上的敏感数据，防止信息泄漏（需启用 CONFIG_GCC_PLUGIN_STACKLEAK）。

1. **切换用户态页表**

    ```assembly
    SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi
    ```

    - 作用：将 CR3 切换到用户态页表（若启用了 PTI），隔离内核与用户地址空间。

1. **恢复栈与返回**

    ```assembly
        popq	%rdi
	    popq	%rsp
    SYM_INNER_LABEL(entry_SYSRETQ_unsafe_stack, SYM_L_GLOBAL)
	    ANNOTATE_NOENDBR
	    swapgs
	    sysretq
    SYM_INNER_LABEL(entry_SYSRETQ_end, SYM_L_GLOBAL)
	    ANNOTATE_NOENDBR
	    int3
    SYM_CODE_END(entry_SYSCALL_64)
    ```
    - 恢复栈指针：从 trampoline 栈恢复用户态 RSP 和 RDI。

    - swapgs: 切换 GS 寄存器基地址回用户态。

    - sysret: 快速返回到用户态，从 RCX 加载 RIP，R11 加载 RFLAGS。

## 14.3 关键涉及思想

1. **性能优先**

    - 优先尝试 SYSRET 快速路径，减少上下文切换开销。

    - 仅在必要时回退到 IRET（慢速路径）。

1. **安全性保障**

    - 严格检查返回地址规范性和段寄存器值，防止恶意利用。

    - 清除栈敏感数据（STACKLEAK_ERASE_NOCLOBBER）和切换 CR3（PTI），防御侧信道攻击。

1. **硬件特性适配**

    - 通过 ALTERNATIVE 宏动态选择代码路径，兼容不同 CPU（如 Xen PV、5 级页表）。